{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Q-Learning with OpenAI Gym\n",
    "The short script aims to help acquaint with implement reinforcement Q-learning using OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description: Self-Driving Taxi\n",
    "\n",
    "#### State Space: There are four locations that we can pick up and drop off a passenger with label R, G, Y, B on the grid\n",
    "* The BLUE letter indicates the pickup location\n",
    "* The MAGENTA letter indicates the drop off location\n",
    "* The solid lines represent walls that the taxi cannnot cross\n",
    "* The filled rectangle represents the taxi itself (yellow when empty and green when carrying passenger)\n",
    "\n",
    "#### Action space: There are six possible actions\n",
    "* Go south\n",
    "* Go north\n",
    "* Go east\n",
    "* Go west\n",
    "* Pickup\n",
    "* Dropoff\n",
    "\n",
    "#### Reward / Penalty:\n",
    "* +20 points for a successful dropoff\n",
    "* Lose 1 point for every timestep it takes\n",
    "* 10 point penalty for illegal pick-up and drop-off actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Adapted From / Sources: \n",
    "* https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "* https://github.com/felipelodur/Q-Learning-Taxi-v2/blob/master/Q-Learning-Taxi.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure to have gym installed:\n",
    "* conda install -c conda-forge gym <b>OR</b>\n",
    "* pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Enviroment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_env = gym.make(\"Taxi-v3\").env\n",
    "taxi_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure the Initial State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (taxi row, taxi column, passenger index, destination index)\n",
    "initial_state = taxi_env.encode(2, 3, 2, 0)\n",
    "\n",
    "taxi_env.s = initial_state\n",
    "taxi_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 368, -1, False)],\n",
       " 1: [(1.0, 168, -1, False)],\n",
       " 2: [(1.0, 288, -1, False)],\n",
       " 3: [(1.0, 248, -1, False)],\n",
       " 4: [(1.0, 268, -10, False)],\n",
       " 5: [(1.0, 268, -10, False)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reward Table\n",
    "\n",
    "# {action: [(probability, nextstate, reward, done)]} for the current state\n",
    "# For the action, [\"Go south\", \"Go north\", \"Go east\", \"Go west\", \"Pickup\", \"Dropoff\"]\n",
    "taxi_env.P[initial_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Using Brute-Force Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 1455\n",
      "Penalties incurred: 491\n"
     ]
    }
   ],
   "source": [
    "taxi_env.s = 328        # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "bf_frames = []          # stored data to use to show animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = taxi_env.action_space.sample()\n",
    "    state, reward, done, info = taxi_env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    bf_frames.append({\n",
    "        'frame': taxi_env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "    })\n",
    "    \n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 1455\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "# Show Animation (Takes awhile)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(bf_frames):\n",
    "    for i, frame in enumerate(bf_frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(bf_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Using Reinforcement Learning (Q-Learning Algorithm)\n",
    "\n",
    "#### Q-Table: \n",
    "The Q-table is a matrix where we have a row for every state (5x5x5x4 = 500) and a column for every action (6). It is use to store Q-value which is a state-action pair (the sum of the instant reward and the discounted future reward). After finished the training process, we can use the Q-value to pick the most optimal action in a given state:\n",
    "* Taxi location: 5 x 5 (grid environment)\n",
    "* Passenger location: 5 (R, G, Y, B, inside the car)\n",
    "* Drop off location: 4 (R, G, Y, B) \n",
    "\n",
    "#### Q-Learning Process:\n",
    "* Initialize the Q-table by all zeros.\n",
    "* Start exploring actions: For each state, select any one among all possible actions for the current state (S).\n",
    "* Travel to the next state (S') as a result of that action (a).\n",
    "* For all possible actions from the state (S') select the one with the highest Q-value.\n",
    "* Update Q-table values using the equation.\n",
    "* Set the next state as the current state.\n",
    "* If goal state is reached, then end and repeat the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Intialized the Q-Table (500Ã—6 matrix of zeros)\n",
    "q_table = np.zeros([taxi_env.observation_space.n, taxi_env.action_space.n])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Configuration\n",
    "\n",
    "alpha = 0.1    # Learning Rate\n",
    "gamma = 0.6    # Discount Factor\n",
    "epsilon = 0.1  # Exploration\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Agent\n",
    "# Note: Clean up the original code (removed unused variables)\n",
    "\n",
    "for run in range(epochs):\n",
    "    state = taxi_env.reset()\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = taxi_env.action_space.sample()   # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])        # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = taxi_env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "print(\"Training Finished\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "# Now that the agent finished training, lets check with our initial setup we have in step 3\n",
    "taxi_env.s = initial_state\n",
    "taxi_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.41021921, -2.40348414, -2.41508538, -2.3639511 , -7.30334373,\n",
       "       -8.66553637])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action = [\"Go south\", \"Go north\", \"Go east\", \"Go west\", \"Pickup\", \"Dropoff\"]\n",
    "# Based on the result, the taxi should go WEST since it has the highest reward\n",
    "q_table[initial_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip Number 5 Length/Step 12\n",
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the Q-table in action with the default initial state\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "for trip_num in range(1, 6):\n",
    "    state = taxi_env.reset()\n",
    "   \n",
    "    done = False\n",
    "    trip_length = 0\n",
    "    \n",
    "    while not done and trip_length < 25:\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, done, info = taxi_env.step(action)\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(\"Trip Number \" + str(trip_num) + \" Length/Step \" + str(trip_length))\n",
    "        print(taxi_env.render(mode='ansi'))\n",
    "        \n",
    "        sleep(.5)\n",
    "        state = next_state\n",
    "        trip_length += 1\n",
    "        \n",
    "    sleep(2)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
